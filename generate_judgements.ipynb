{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Speed UP Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "max_model_len, tp_size = 4096, 1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "def get_response_prompts(instructions):\n",
    "    positive_prompts = []\n",
    "    for instruction in instructions:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction}\n",
    "        ]\n",
    "        positive_prompts.append(messages)\n",
    "    return positive_prompts\n",
    "\n",
    "def get_worse_response_question_prompts(instructions, positive_responses, worse_response_prompt_template):\n",
    "    worse_prompts = []\n",
    "    for instruction, response in zip(instructions, positive_responses):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": worse_response_prompt_template.format(input=instruction, generation=response)}\n",
    "        ]\n",
    "        worse_prompts.append(messages)\n",
    "    return worse_prompts\n",
    "\n",
    "def extract_negative_questions(instructions, positive_responses, inputs):\n",
    "    all_negative_instructions = []\n",
    "    temp_instructions = []\n",
    "    temp_positive_responses = []\n",
    "    for idx, item in enumerate(inputs):\n",
    "        try:\n",
    "            start = item.index(\"[The start of User Question]\")+len(\"[The start of User Question] \")\n",
    "            if(\"[The end of User Question]\" in item):\n",
    "                end = item.index(\"[The end of User Question]\")\n",
    "            else:\n",
    "                end = len(item)\n",
    "            question = item[start:end]\n",
    "            all_negative_instructions.append(question)\n",
    "            temp_instructions.append(instructions[idx])\n",
    "            temp_positive_responses.append(positive_responses[idx])\n",
    "        except:\n",
    "            print(\"Generation String Not Found! ID:\", idx)\n",
    "            # print(negative)\n",
    "    return temp_instructions, temp_positive_responses, all_negative_instructions\n",
    "\n",
    "def extract_negative_responses(instructions, positive_responses, generated_text):\n",
    "    all_responses = []\n",
    "    for idx, (instruction, positive, negative) in enumerate(zip(instructions, positive_responses, generated_text)):\n",
    "        try:\n",
    "            start = negative.index(\"[The start of Modified Instruction Response]\")+len(\"[The start of Modified Instruction Response] \")\n",
    "            if(\"[The end of Modified Instruction Response]\" in negative):\n",
    "                end = negative.index(\"[The end of Modified Instruction Response]\")\n",
    "            else:\n",
    "                end = len(negative)\n",
    "            negative = negative[start:end]\n",
    "            all_responses.append({\"instruction\": instruction, \"positive\": positive, \"negative\": negative})\n",
    "        except:\n",
    "            print(\"Generation String Not Found! ID:\", idx)\n",
    "            # print(negative)\n",
    "    return all_responses\n",
    "\n",
    "def create_judge_response_prompts(instructions, positives, negatives, judging_template):\n",
    "    judge_prompts = []\n",
    "    labels = []\n",
    "    for instruction, positive, negative in zip(instructions, positives, negatives):\n",
    "        rand_num = np.random.rand()\n",
    "        if(rand_num < 0.5):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                    generation=positive, \n",
    "                                                                    generation2=negative)},\n",
    "            ]\n",
    "            judge_prompts.append(messages)\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                    generation=negative, \n",
    "                                                                    generation2=positive)},\n",
    "            ]\n",
    "            judge_prompts.append(messages)\n",
    "            labels.append(0)\n",
    "    return judge_prompts, labels\n",
    "\n",
    "def load_textfile_as_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=2000)\n",
    "llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, \n",
    "          trust_remote_code=True, enforce_eager=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "#dataset is a subset of WildChat\n",
    "dataset = load_dataset(\"facebook/Self-taught-evaluator-DPO-data\")\n",
    "WildChat = load_dataset(\"allenai/WildChat-1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get hashes\n",
    "#load the hash_id2content dictionary\n",
    "# with open(\"hash_id2content.pkl\", \"rb\") as f:\n",
    "#   hash_id2content = pickle.load(f)\n",
    "hash_id2content = dict()\n",
    "for ex in tqdm(WildChat[\"train\"], total=len(WildChat[\"train\"])):\n",
    "  turn = ex[\"turn\"]\n",
    "  hash_id2content[ex[\"conversation_hash\"]] = ex[\"conversation\"][2 * (turn - 1)][\"content\"]\n",
    "\n",
    "\n",
    "print(\"Starting 2.\")\n",
    "train_data = []\n",
    "for ex in tqdm(dataset[\"train\"], total=len(dataset[\"train\"])):\n",
    "  if ex[\"instruction\"] not in hash_id2content:\n",
    "    continue\n",
    "  else:\n",
    "    ex[\"src\"] = ex[\"src\"].replace(ex[\"instruction\"], hash_id2content[ex[\"instruction\"]])\n",
    "    train_data.append(ex)\n",
    "\n",
    "print(\"Starting 3.\")\n",
    "# Extract Instructions\n",
    "skip = 6 # I found that the dataset instructions has the same instruction repeated 6 times? Not sure why. \n",
    "all_instructions = []\n",
    "num_responses = len(dataset['train'])//skip\n",
    "for i in tqdm(range(0, num_responses*skip, skip), total=num_responses):\n",
    "    try:\n",
    "        instruction_example = hash_id2content[dataset['train'][i]['instruction']]\n",
    "        all_instructions.append(instruction_example)\n",
    "    except:\n",
    "        continue\n",
    "print(all_instructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_str = \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "eot_str = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "user_str = \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "\n",
    "prompts = []\n",
    "for sample in tqdm(dataset['train'], total=len(dataset['train'])):\n",
    "    judgement_prompt = sample[\"src\"]\n",
    "    judgement_prompt = judgement_prompt.replace(sys_str, \"\")\n",
    "    sys_idx = judgement_prompt.index(user_str)\n",
    "    system_prompt = judgement_prompt[:sys_idx]\n",
    "    judgement_prompt = judgement_prompt.replace(user_str, \"\")\n",
    "    judgement_prompt = judgement_prompt[sys_idx:]\n",
    "    judgement_prompt = judgement_prompt.replace(eot_str, \"\")\n",
    "    if sample[\"instruction\"] not in hash_id2content:\n",
    "        continue\n",
    "    else:\n",
    "        hash_id = sample[\"instruction\"]\n",
    "        instruction = hash_id2content[hash_id]\n",
    "        judgement_prompt = judgement_prompt.replace(hash_id, instruction)\n",
    "    answer = sample['tgt_chosen']\n",
    "    if(\"[[A]]\" in answer):\n",
    "        label = 1\n",
    "    elif(\"[[B]]\" in answer):\n",
    "        label = 0\n",
    "    else:\n",
    "        continue\n",
    "    # print(\"System____\")\n",
    "    # print(system_prompt)\n",
    "    # print(\"Judgement____\")\n",
    "    # print(judgement_prompt)\n",
    "    # print(\"____\")\n",
    "    # print(label)\n",
    "    # print(answer)\n",
    "    prompts.append({\"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                 {\"role\": \"user\", \"content\": judgement_prompt}],\n",
    "                    \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the list to a pickle file\n",
    "with open(\"prompt_pre_judge.pkl\", \"wb\") as file:\n",
    "    pickle.dump(prompts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "with open(\"prompt_pre_judge.pkl\", \"rb\") as file:\n",
    "    loaded_list = pickle.load(file)\n",
    "\n",
    "print(loaded_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = loaded_list[::4]\n",
    "print(len(filtered_list), len(loaded_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_token_ids = [tokenizer.apply_chat_template(messages['messages'], add_generation_prompt=True, tokenize=True) for messages in tqdm(filtered_list, total=len(filtered_list))]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_judgements = [output.outputs[0].text for output in outputs]\n",
    "print(generated_judgements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list[9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_judgements), generated_judgements[9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = 0\n",
    "A_correct = []\n",
    "B_correct = []\n",
    "count_correct_format_incorrect_answer = 0\n",
    "for judge_info, judgement in zip(filtered_list, generated_judgements):\n",
    "    label = judge_info['label']\n",
    "    messages = judge_info['messages']\n",
    "    messages = judge_info['messages'][0:2]\n",
    "    if((\"[[A]]\" in judgement and label == 1) or (\"[[B]]\" in judgement and label == 0)):\n",
    "        count_correct += 1\n",
    "        messages.append({\"role\": \"assistant\", \"content\": judgement})\n",
    "        if(\"[[A]]\" in judgement):\n",
    "            A_correct.append(messages)\n",
    "        else:\n",
    "            B_correct.append(messages)\n",
    "    else:\n",
    "        # print(judgement)\n",
    "        # print(\"*************\")\n",
    "        if(\"[[A]]\" in judgement or \"[[B]]\" in judgement):\n",
    "            count_correct_format_incorrect_answer += 1\n",
    "print(\"Correct\", count_correct, \"/\", len(generated_judgements))\n",
    "print(\"A correct: \", len(A_correct))\n",
    "print(\"B correct: \", len(B_correct))\n",
    "print(\"Formatted Incorrect: \", count_correct_format_incorrect_answer, \"/\", len(generated_judgements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "import random\n",
    "if(len(A_correct) > len(B_correct)):\n",
    "    A_correct_sampled = random.sample(A_correct, len(B_correct))\n",
    "    B_correct_sampled = B_correct\n",
    "else:\n",
    "    B_correct_sampled = random.sample(B_correct, len(A_correct))\n",
    "    A_correct_sampled = A_correct\n",
    "\n",
    "print(len(A_correct_sampled), len(B_correct_sampled))\n",
    "\n",
    "#calculate token statistics for the generated responses\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_token_statistics(messages):\n",
    "    token_counts = []\n",
    "    for message in messages:\n",
    "        tokens = tokenizer.apply_chat_template(message, add_generation_prompt=False, tokenize=True)\n",
    "        token_counts.append(len(tokens))\n",
    "    print(\"Mean Tokens: \", np.mean(token_counts))\n",
    "    print(\"Median Tokens: \", np.median(token_counts))\n",
    "    print(\"Max Tokens: \", np.max(token_counts))\n",
    "    print(\"Min Tokens: \", np.min(token_counts))\n",
    "    print(\"STD Tokens: \", np.std(token_counts))\n",
    "\n",
    "calculate_token_statistics(A_correct_sampled+ B_correct_sampled)\n",
    "\n",
    "# Save the dataset\n",
    "import json\n",
    "\n",
    "all_sft_samples = A_correct_sampled + B_correct_sampled\n",
    "json_array = []\n",
    "for i, judgement in enumerate(all_sft_samples):\n",
    "    json_array.append({\"messages\": judgement})\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = 'all_judgements_phi.json'\n",
    "\n",
    "# Write the list of dictionaries to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_array, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_response_prompt_template = load_textfile_as_string('./prompts/worse_response_v2.prompt')\n",
    "print(worse_response_prompt_template)\n",
    "judging_prompt_template = load_textfile_as_string('./prompts/eval_plan.prompt')\n",
    "print(judging_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses_pos_prompts = get_response_prompts(all_instructions)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_responses_pos = [output.outputs[0].text for output in outputs]\n",
    "print(generated_responses_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions_pos_neg = get_worse_response_question_prompts(all_instructions, generated_responses_pos, worse_response_prompt_template)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in all_instructions_pos_neg]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "all_instructions_neg = [output.outputs[0].text for output in outputs]\n",
    "print(all_instructions_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_all_instructions, f_generated_responses_pos, f_all_instructions_neg = extract_negative_questions(all_instructions, generated_responses_pos, all_instructions_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_instructions), len(f_all_instructions), len(f_generated_responses_pos), len(f_all_instructions_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_to_judge = extract_negative_responses(all_instructions, generated_responses_pos, generated_responses_neg)\n",
    "# print(items_to_judge[0])\n",
    "# print(len(items_to_judge))\n",
    "\n",
    "generated_responses_neg_prompts = get_response_prompts(f_all_instructions_neg)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_neg_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "f_generated_responses_neg = [output.outputs[0].text for output in outputs]\n",
    "print(f_generated_responses_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_judgements, labels = create_judge_response_prompts(f_all_instructions, f_generated_responses_pos, f_generated_responses_neg, judging_prompt_template)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in all_judgements]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_judgements = [output.outputs[0].text for output in outputs]\n",
    "print(generated_judgements[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(f_all_instructions[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_generated_responses_pos[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_all_instructions_neg[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_generated_responses_neg[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = 0\n",
    "A_correct = []\n",
    "B_correct = []\n",
    "count_correct_format_incorrect_answer = 0\n",
    "for judge_prompt, judgement, label in zip(all_judgements, generated_judgements, labels):\n",
    "    if((\"[[A]]\" in judgement and label == 1) or (\"[[B]]\" in judgement and label == 0)):\n",
    "        count_correct += 1\n",
    "        if(\"[[A]]\" in judgement):\n",
    "            A_correct.append([{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "                                {\"role\": \"user\", \"content\": judge_prompt[1][\"content\"]},\n",
    "                                {\"role\": \"assistant\", \"content\": judgement}])\n",
    "        else:\n",
    "            B_correct.append([{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "                                {\"role\": \"user\", \"content\": judge_prompt[1][\"content\"]},\n",
    "                                {\"role\": \"assistant\", \"content\": judgement}])\n",
    "    else:\n",
    "        # print(judgement)\n",
    "        # print(\"*************\")\n",
    "        if(\"[[A]]\" in judgement or \"[[B]]\" in judgement):\n",
    "            count_correct_format_incorrect_answer += 1\n",
    "print(\"Correct\", count_correct, \"/\", len(all_judgements))\n",
    "print(\"A correct: \", len(A_correct))\n",
    "print(\"B correct: \", len(B_correct))\n",
    "print(\"Formatted Incorrect: \", count_correct_format_incorrect_answer, \"/\", len(all_judgements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "import random\n",
    "if(len(A_correct) > len(B_correct)):\n",
    "    A_correct_sampled = random.sample(A_correct, len(B_correct))\n",
    "    B_correct_sampled = B_correct\n",
    "else:\n",
    "    B_correct_sampled = random.sample(B_correct, len(A_correct))\n",
    "    A_correct_sampled = A_correct\n",
    "\n",
    "#calculate token statistics for the generated responses\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_token_statistics(messages):\n",
    "    token_counts = []\n",
    "    for message in messages:\n",
    "        tokens = tokenizer.apply_chat_template(message, add_generation_prompt=False, tokenize=True)\n",
    "        token_counts.append(len(tokens))\n",
    "    print(\"Mean Tokens: \", np.mean(token_counts))\n",
    "    print(\"Median Tokens: \", np.median(token_counts))\n",
    "    print(\"Max Tokens: \", np.max(token_counts))\n",
    "    print(\"Min Tokens: \", np.min(token_counts))\n",
    "    print(\"STD Tokens: \", np.std(token_counts))\n",
    "\n",
    "calculate_token_statistics(A_correct_sampled+ B_correct_sampled)\n",
    "\n",
    "# Save the dataset\n",
    "import json\n",
    "\n",
    "all_sft_samples = A_correct_sampled + B_correct_sampled\n",
    "json_array = []\n",
    "for i, judgement in enumerate(all_sft_samples):\n",
    "    json_array.append({\"messages\": judgement})\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = 'all_judgements_llama.json'\n",
    "\n",
    "# Write the list of dictionaries to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_array, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(A_correct_sampled), len(B_correct_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate token statistics for the generated responses\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_token_statistics(messages):\n",
    "    token_counts = []\n",
    "    for message in messages:\n",
    "        tokens = tokenizer.apply_chat_template(message, add_generation_prompt=False, tokenize=True)\n",
    "        token_counts.append(len(tokens))\n",
    "    print(\"Mean Tokens: \", np.mean(token_counts))\n",
    "    print(\"Median Tokens: \", np.median(token_counts))\n",
    "    print(\"Max Tokens: \", np.max(token_counts))\n",
    "    print(\"Min Tokens: \", np.min(token_counts))\n",
    "    print(\"STD Tokens: \", np.std(token_counts))\n",
    "\n",
    "calculate_token_statistics(A_correct_sampled+ B_correct_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "import json\n",
    "\n",
    "all_sft_samples = A_correct_sampled + B_correct_sampled\n",
    "json_array = []\n",
    "for i, judgement in enumerate(all_sft_samples):\n",
    "    json_array.append({\"messages\": judgement})\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = 'all_judgements_llama.json'\n",
    "\n",
    "# Write the list of dictionaries to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_array, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
