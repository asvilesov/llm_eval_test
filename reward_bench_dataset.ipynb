{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389d4e8-4b23-4344-b0ad-58c225322dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp import ShardingStrategy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import LoraConfig, PeftConfig, PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "import gc\n",
    "\n",
    "max_model_len, tp_size = 4096, 1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Eval Plan\n",
    "def evaluate_responses(generated_judgements, labels, dataset_types):\n",
    "    correct = 0\n",
    "    correct_A = 0\n",
    "    correct_B = 0\n",
    "    unstructered = 0\n",
    "    dataset_accs = {}\n",
    "    for i, judgement in enumerate(generated_judgements):\n",
    "        d_type = dataset_types[i]\n",
    "        if(d_type not in dataset_accs.keys()):\n",
    "            dataset_accs[d_type] = []\n",
    "            \n",
    "        if(labels[i] == 1 and \"[[A]]\" in judgement):\n",
    "            correct += 1\n",
    "            correct_A += 1\n",
    "            dataset_accs[d_type].append(1)\n",
    "        elif(labels[i] == 0 and \"[[B]]\" in judgement):\n",
    "            correct += 1\n",
    "            correct_B += 1\n",
    "            dataset_accs[d_type].append(1)\n",
    "        elif(\"[[A]]\" not in judgement and \"[[B]]\" not in judgement):\n",
    "            unstructered += 1\n",
    "            dataset_accs[d_type].append(0)\n",
    "        else:\n",
    "            dataset_accs[d_type].append(-1)\n",
    "    print(f\"Correct: {correct}, Correct_A: {correct_A}, Correct_B: {correct_B}, Unstructured: {unstructered}\")\n",
    "    acc = correct/len(labels)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    acc_corrected = correct/(len(labels) - unstructered)\n",
    "    print(f\"Corrected Accuracy: {acc_corrected}\")\n",
    "\n",
    "    for key in dataset_accs:\n",
    "        acc1, acc2 = calculate_accuracies(dataset_accs[key])\n",
    "        print(f\"{key}: Acc Corrected={acc1}, Acc Total={acc2}\")\n",
    "    \n",
    "    return acc, acc_corrected, unstructered, dataset_accs\n",
    "    \n",
    "\n",
    "def load_textfile_as_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def create_rewardbench_jsonl(judge_prompt_file, system_based=True):\n",
    "    from datasets import load_dataset\n",
    "    import numpy as np\n",
    "    ds = load_dataset(\"allenai/reward-bench\")\n",
    "\n",
    "    judging_template = load_textfile_as_string(judge_prompt_file)\n",
    "\n",
    "    judge_prompts = []\n",
    "    labels = []\n",
    "    dataset_type = []\n",
    "    for idx, point in enumerate(ds['filtered']):\n",
    "        instruction = point['prompt']\n",
    "        if(np.random.rand() < 0.5):\n",
    "            response_A = point['chosen']\n",
    "            response_B = point['rejected']\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            response_A = point['rejected']\n",
    "            response_B = point['chosen']\n",
    "            labels.append(0)\n",
    "        if(system_based):\n",
    "            system_prompt = \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \\\"[[A]]\\\" if assistant A is better, \\\"[[B]]\\\" if assistant B is better.\"\n",
    "            messages = [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                            generation=response_A, \n",
    "                                                                            generation2=response_B)},\n",
    "                    ]\n",
    "            \n",
    "        else:\n",
    "            messages = [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                            generation=response_A, \n",
    "                                                                            generation2=response_B)},\n",
    "                    ]\n",
    "        judge_prompts.append(messages)\n",
    "        dataset_type.append(point['subset'])\n",
    "        \n",
    "    return judge_prompts, labels, dataset_type\n",
    "\n",
    "def calculate_accuracies(predictions):\n",
    "    \"\"\"\n",
    "    Calculate two accuracies:\n",
    "    - Accuracy 1: 1 is correct, 0 and -1 are incorrect.\n",
    "    - Accuracy 2: 1 is correct, 0 and -1 are both incorrect.\n",
    "    \n",
    "    Args:\n",
    "    - predictions (list): List of predictions containing 0, 1, and -1.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing both accuracies.\n",
    "    \"\"\"\n",
    "    total = len(predictions)\n",
    "    if total == 0:\n",
    "        return {\"accuracy_1\": 0.0, \"accuracy_2\": 0.0}\n",
    "\n",
    "    # Accuracy 1: 1 is correct, 0 and -1 are incorrect.\n",
    "    correct_1 = sum(1 for pred in predictions if pred == 1)\n",
    "    accuracy_1 = correct_1 / total\n",
    "\n",
    "    # Accuracy 2: 1 is correct, -1 is also incorrect (0 and -1 are incorrect).\n",
    "    incorrect_2 = sum(1 for pred in predictions if pred == 0)\n",
    "    accuracy_2 = correct_1 / (correct_1+incorrect_2)  # Same calculation since both consider 1 as correct.\n",
    "\n",
    "    return accuracy_1, accuracy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390d7c3-559e-48f3-9abf-71cdd252171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "ds = load_dataset(\"allenai/reward-bench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924a277-e6bb-440b-91e7-43097761c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ds['filtered'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9c295-d39e-4160-bd36-2468d755f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompts, labels, dataset_type = create_rewardbench_jsonl(\"./prompts/eval_plan_sys.prompt\", system_based=True)\n",
    "print(judge_prompts[0], labels[0], dataset_type[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc665ec5-49ba-4160-83af-fd4a19b90281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
