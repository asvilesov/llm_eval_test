{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Speed UP Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "max_model_len, tp_size = 8192, 1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "def get_response_prompts(instructions):\n",
    "    positive_prompts = []\n",
    "    for instruction in instructions:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction}\n",
    "        ]\n",
    "        positive_prompts.append(messages)\n",
    "    return positive_prompts\n",
    "\n",
    "def get_worse_response_question_prompts(instructions, positive_responses, worse_response_prompt_template):\n",
    "    worse_prompts = []\n",
    "    for instruction, response in zip(instructions, positive_responses):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": worse_response_prompt_template.format(input=instruction, generation=response)}\n",
    "        ]\n",
    "        worse_prompts.append(messages)\n",
    "    return worse_prompts\n",
    "\n",
    "def extract_negative_questions(instructions, positive_responses, inputs):\n",
    "    all_negative_instructions = []\n",
    "    temp_instructions = []\n",
    "    temp_positive_responses = []\n",
    "    for idx, item in enumerate(inputs):\n",
    "        try:\n",
    "            start = item.index(\"[The start of User Question]\")+len(\"[The start of User Question] \")\n",
    "            if(\"[The end of User Question]\" in item):\n",
    "                end = item.index(\"[The end of User Question]\")\n",
    "            else:\n",
    "                end = len(item)\n",
    "            question = item[start:end]\n",
    "            all_negative_instructions.append(question)\n",
    "            temp_instructions.append(instructions[idx])\n",
    "            temp_positive_responses.append(positive_responses[idx])\n",
    "        except:\n",
    "            print(\"Generation String Not Found! ID:\", idx)\n",
    "            # print(negative)\n",
    "    return temp_instructions, temp_positive_responses, all_negative_instructions\n",
    "\n",
    "def extract_negative_responses(instructions, positive_responses, generated_text):\n",
    "    all_responses = []\n",
    "    for idx, (instruction, positive, negative) in enumerate(zip(instructions, positive_responses, generated_text)):\n",
    "        try:\n",
    "            start = negative.index(\"[The start of Modified Instruction Response]\")+len(\"[The start of Modified Instruction Response] \")\n",
    "            if(\"[The end of Modified Instruction Response]\" in negative):\n",
    "                end = negative.index(\"[The end of Modified Instruction Response]\")\n",
    "            else:\n",
    "                end = len(negative)\n",
    "            negative = negative[start:end]\n",
    "            all_responses.append({\"instruction\": instruction, \"positive\": positive, \"negative\": negative})\n",
    "        except:\n",
    "            print(\"Generation String Not Found! ID:\", idx)\n",
    "            # print(negative)\n",
    "    return all_responses\n",
    "\n",
    "def create_judge_response_prompts(instructions, positives, negatives, judging_template):\n",
    "    judge_prompts = []\n",
    "    labels = []\n",
    "    for instruction, positive, negative in zip(instructions, positives, negatives):\n",
    "        rand_num = np.random.rand()\n",
    "        if(rand_num < 0.5):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                    generation=positive, \n",
    "                                                                    generation2=negative)},\n",
    "            ]\n",
    "            judge_prompts.append(messages)\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                    generation=negative, \n",
    "                                                                    generation2=positive)},\n",
    "            ]\n",
    "            judge_prompts.append(messages)\n",
    "            labels.append(0)\n",
    "    return judge_prompts, labels\n",
    "\n",
    "def load_textfile_as_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Begin your evaluation by first verifying whether each response contains any obvious or subtle errors. Then propose an appropriate evaluaiton rubric, e.g. 1-5 criteria that are important for evaluating responses to this specific user question. Continue your evaluation by checking each response carefully along those criteria. Based on the analysis in previous steps, choose which response is better overall. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your evaluation, output your final verdict by strictly following this format: \\\\\\\"[[A]]\\\\\\\" if assistant A is better, \\\\\\\"[[B]]\\\\\\\" if assistant B is better. \\n\\n[User Question]\\nfast reading method when first few letters of words are bold\\n\\n[The Start of Assistant A's Answer]\\nYou're referring to a technique called \\\"Rapid Serial Visual Presentation\\\" (RSVP) with bolded first few letters! This method can indeed help you read faster and more efficiently. Here's how it works:\\n\\n1. Identify the first few letters of each word that are bolded. This can be 2-3 letters, depending on your preference.\\n2. Focus on the bolded letters to quickly identify the word. This helps your brain to recognize the word more quickly and accurately.\\n3. Move your eyes to the next word, repeating the process of identifying the bolded letters and recognizing the word.\\n4. Continue this process at a rapid pace, moving your eyes from one word to the next.\\n\\nThis technique can help you read faster in several ways:\\n\\n* Reduced eye movement: By focusing on the bolded letters, you reduce the amount of eye movement needed to read each word, allowing you to move more quickly through the text.\\n* Increased recognition: The bolded letters help your brain to recognize words more quickly, as your brain is able to latch onto the familiar pattern of letters.\\n* Improved processing: The bolded letters can also help your brain to process the words more efficiently, as the bolded letters provide a clear cue for your brain to recognize the word.\\n\\nTo get the most out of this technique, try the following:\\n\\n* Start with a moderate pace and gradually increase your speed as you become more comfortable with the technique.\\n* Practice with different texts and materials to get a feel for how the technique works with different types of writing.\\n* Experiment with different bolding patterns, such as bolding the first letter of each word or the first letter of every other word, to find what works best for you.\\n\\nRemember, the key to success with this technique is to practice regularly and develop a consistent reading rhythm. With time and practice, you can improve your reading speed and comprehension using this method!\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\nYou're interested in learning a fast reading method that utilizes visual cues to improve recognition and processing! I'd be happy to help you with that.\\n\\nThe method I'd like to introduce you to is called the \\\"Meta-Guiding\\\" technique. It's a visual cue-based method that can help you read faster and more efficiently.\\n\\n**What is Meta-Guiding?**\\n\\nMeta-Guiding is a technique that uses visual cues to guide your eyes as you read. The goal is to train your eyes to move more efficiently across the page, recognizing words and phrases more quickly, and processing the information more effectively.\\n\\n**How to Implement Meta-Guiding:**\\n\\n1. **Start by choosing a text**: Select a passage that you want to read quickly and efficiently. It can be a short article, a chapter, or even a book.\\n2. **Identify the visual cues**: As you read, pay attention to the visual cues that catch your attention. These can be:\\n\\t* Headings and subheadings\\n\\t* Bold or italicized text\\n\\t* Images or graphics\\n\\t* Quotes or dialogue\\n\\t* Numbered or bulleted lists\\n3. **Use these cues as guides**: As you read, use these visual cues to guide your eyes across the page. For example, if you see a heading, use it as a reference point to scan the surrounding text.\\n4. **Practice active reading**: As you read, ask yourself questions like:\\n\\t* What is the main idea of this section?\\n\\t* What is the author trying to say?\\n\\t* What are the key points being made?\\n5. **Use your peripheral vision**: Instead of focusing on individual words, use your peripheral vision to take in larger chunks of text. This will help you recognize patterns and relationships between words.\\n6. **Practice, practice, practice!**: The more you practice using visual cues to guide your reading, the more efficient and effective you'll become.\\n\\n**Benefits of Meta-Guiding:**\\n\\n1. **Faster reading speeds**: By using visual cues, you can train your eyes to move more quickly across the page, allowing you to read faster.\\n2. **Improved comprehension**: By focusing on the main ideas and key points, you'll be able to comprehend the text more effectively.\\n3. **Reduced eye fatigue**: By using your peripheral vision and focusing on visual cues, you'll reduce eye fatigue and strain.\\n4. **Increased retention**: By actively engaging with the text and using visual cues, you'll be able to retain more information.\\n\\n**Tips and Variations:**\\n\\n1. **Start with short texts**: Begin with shorter texts and gradually move to longer ones as you become more comfortable with the technique.\\n2. **Experiment with different types of text**: Try using Meta-Guiding with different types of text, such as articles, blog posts, or even social media updates.\\n3. **Combine with other reading techniques**: You can combine Meta-Guiding with other reading techniques, such as skimming or scanning, to further improve your reading efficiency.\\n\\nI hope this helps you get started with the Meta-Guiding technique! Remember to practice regularly and have fun exploring the benefits of this visual cue-based method.\\n[The End of Assistant B's Answer]\"\n",
    "            }\n",
    "        ]\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt_token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(prompt_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=2000)\n",
    "llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, \n",
    "          trust_remote_code=True, enforce_eager=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "#dataset is a subset of WildChat\n",
    "dataset = load_dataset(\"facebook/Self-taught-evaluator-DPO-data\")\n",
    "WildChat = load_dataset(\"allenai/WildChat-1M\")\n",
    "\n",
    "# get hashes\n",
    "#load the hash_id2content dictionary\n",
    "# with open(\"hash_id2content.pkl\", \"rb\") as f:\n",
    "#   hash_id2content = pickle.load(f)\n",
    "hash_id2content = dict()\n",
    "for ex in tqdm(WildChat[\"train\"], total=len(WildChat[\"train\"])):\n",
    "  turn = ex[\"turn\"]\n",
    "  hash_id2content[ex[\"conversation_hash\"]] = ex[\"conversation\"][2 * (turn - 1)][\"content\"]\n",
    "\n",
    "\n",
    "print(\"Starting 2.\")\n",
    "train_data = []\n",
    "for ex in tqdm(dataset[\"train\"], total=len(dataset[\"train\"])):\n",
    "  if ex[\"instruction\"] not in hash_id2content:\n",
    "    continue\n",
    "  else:\n",
    "    ex[\"src\"] = ex[\"src\"].replace(ex[\"instruction\"], hash_id2content[ex[\"instruction\"]])\n",
    "    train_data.append(ex)\n",
    "\n",
    "print(\"Starting 3.\")\n",
    "# Extract Instructions\n",
    "skip = 6 # I found that the dataset instructions has the same instruction repeated 6 times? Not sure why. \n",
    "all_instructions = []\n",
    "num_responses = len(dataset['train'])//skip\n",
    "for i in tqdm(range(0, num_responses*skip, skip), total=num_responses):\n",
    "    try:\n",
    "        instruction_example = hash_id2content[dataset['train'][i]['instruction']]\n",
    "        all_instructions.append(instruction_example)\n",
    "    except:\n",
    "        continue\n",
    "print(all_instructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_response_prompt_template = load_textfile_as_string('./prompts/worse_response_v2.prompt')\n",
    "print(worse_response_prompt_template)\n",
    "judging_prompt_template = load_textfile_as_string('./prompts/eval_plan.prompt')\n",
    "print(judging_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses_pos_prompts = get_response_prompts(all_instructions)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_responses_pos = [output.outputs[0].text for output in outputs]\n",
    "print(generated_responses_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions_pos_neg = get_worse_response_question_prompts(all_instructions, generated_responses_pos, worse_response_prompt_template)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in all_instructions_pos_neg]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "all_instructions_neg = [output.outputs[0].text for output in outputs]\n",
    "print(all_instructions_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_all_instructions, f_generated_responses_pos, f_all_instructions_neg = extract_negative_questions(all_instructions, generated_responses_pos, all_instructions_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_instructions), len(f_all_instructions), len(f_generated_responses_pos), len(f_all_instructions_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_to_judge = extract_negative_responses(all_instructions, generated_responses_pos, generated_responses_neg)\n",
    "# print(items_to_judge[0])\n",
    "# print(len(items_to_judge))\n",
    "\n",
    "generated_responses_neg_prompts = get_response_prompts(f_all_instructions_neg)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_neg_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "f_generated_responses_neg = [output.outputs[0].text for output in outputs]\n",
    "print(f_generated_responses_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_judgements, labels = create_judge_response_prompts(f_all_instructions, f_generated_responses_pos, f_generated_responses_neg, judging_prompt_template)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in all_judgements]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_judgements = [output.outputs[0].text for output in outputs]\n",
    "print(generated_judgements[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(f_all_instructions[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_generated_responses_pos[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_all_instructions_neg[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_generated_responses_neg[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = 0\n",
    "A_correct = []\n",
    "B_correct = []\n",
    "count_correct_format_incorrect_answer = 0\n",
    "for judge_prompt, judgement, label in zip(all_judgements, generated_judgements, labels):\n",
    "    if((\"[[A]]\" in judgement and label == 1) or (\"[[B]]\" in judgement and label == 0)):\n",
    "        count_correct += 1\n",
    "        if(\"[[A]]\" in judgement):\n",
    "            A_correct.append([{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "                                {\"role\": \"user\", \"content\": judge_prompt[1][\"content\"]},\n",
    "                                {\"role\": \"assistant\", \"content\": judgement}])\n",
    "        else:\n",
    "            B_correct.append([{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "                                {\"role\": \"user\", \"content\": judge_prompt[1][\"content\"]},\n",
    "                                {\"role\": \"assistant\", \"content\": judgement}])\n",
    "    else:\n",
    "        # print(judgement)\n",
    "        # print(\"*************\")\n",
    "        if(\"[[A]]\" in judgement or \"[[B]]\" in judgement):\n",
    "            count_correct_format_incorrect_answer += 1\n",
    "print(\"Correct\", count_correct, \"/\", len(all_judgements))\n",
    "print(\"A correct: \", len(A_correct))\n",
    "print(\"B correct: \", len(B_correct))\n",
    "print(\"Formatted Incorrect: \", count_correct_format_incorrect_answer, \"/\", len(all_judgements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "import random\n",
    "if(len(A_correct) > len(B_correct)):\n",
    "    A_correct_sampled = random.sample(A_correct, len(B_correct))\n",
    "    B_correct_sampled = B_correct\n",
    "else:\n",
    "    B_correct_sampled = random.sample(B_correct, len(A_correct))\n",
    "    A_correct_sampled = A_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(A_correct_sampled), len(B_correct_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate token statistics for the generated responses\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_token_statistics(messages):\n",
    "    token_counts = []\n",
    "    for message in messages:\n",
    "        tokens = tokenizer.apply_chat_template(message, add_generation_prompt=False, tokenize=True)\n",
    "        token_counts.append(len(tokens))\n",
    "    print(\"Mean Tokens: \", np.mean(token_counts))\n",
    "    print(\"Median Tokens: \", np.median(token_counts))\n",
    "    print(\"Max Tokens: \", np.max(token_counts))\n",
    "    print(\"Min Tokens: \", np.min(token_counts))\n",
    "    print(\"STD Tokens: \", np.std(token_counts))\n",
    "\n",
    "calculate_token_statistics(A_correct_sampled+ B_correct_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "import json\n",
    "\n",
    "all_sft_samples = A_correct_sampled + B_correct_sampled\n",
    "json_array = []\n",
    "for i, judgement in enumerate(all_sft_samples):\n",
    "    json_array.append({\"messages\": judgement})\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = 'all_judgements_llama.json'\n",
    "\n",
    "# Write the list of dictionaries to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_array, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
