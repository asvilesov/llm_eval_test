{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Speed UP Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "from vllm import LLM, SamplingParams\n",
    "max_model_len, tp_size = 4000, 1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "def get_response_prompts(instructions):\n",
    "    positive_prompts = []\n",
    "    for instruction in instructions:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction}\n",
    "        ]\n",
    "        positive_prompts.append(messages)\n",
    "    return positive_prompts\n",
    "\n",
    "def get_worse_response_question_prompts(instructions, positive_responses, worse_response_prompt_template):\n",
    "    worse_prompts = []\n",
    "    for instruction, response in zip(instructions, positive_responses):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": worse_response_prompt_template.format(input=instruction, generation=response)}\n",
    "        ]\n",
    "        worse_prompts.append(messages)\n",
    "    return worse_prompts\n",
    "\n",
    "def extract_negative_questions(instructions, positive_responses, inputs):\n",
    "    all_negative_instructions = []\n",
    "    temp_instructions = []\n",
    "    temp_positive_responses = []\n",
    "    for idx, item in enumerate(inputs):\n",
    "        try:\n",
    "            start = item.index(\"[The start of User Question]\")+len(\"[The start of User Question] \")\n",
    "            if(\"[The end of User Question]\" in item):\n",
    "                end = item.index(\"[The end of User Question]\")\n",
    "            else:\n",
    "                end = len(item)\n",
    "            question = item[start:end]\n",
    "            all_negative_instructions.append(question)\n",
    "            temp_instructions.append(instructions[idx])\n",
    "            temp_positive_responses.append(positive_responses[idx])\n",
    "        except:\n",
    "            print(\"Generation String Not Found! ID:\", idx)\n",
    "            # print(negative)\n",
    "    return temp_instructions, temp_positive_responses, all_negative_instructions\n",
    "\n",
    "def extract_negative_responses(instructions, positive_responses, generated_text):\n",
    "    all_responses = []\n",
    "    for idx, (instruction, positive, negative) in enumerate(zip(instructions, positive_responses, generated_text)):\n",
    "        try:\n",
    "            start = negative.index(\"[The start of Modified Instruction Response]\")+len(\"[The start of Modified Instruction Response] \")\n",
    "            if(\"[The end of Modified Instruction Response]\" in negative):\n",
    "                end = negative.index(\"[The end of Modified Instruction Response]\")\n",
    "            else:\n",
    "                end = len(negative)\n",
    "            negative = negative[start:end]\n",
    "            all_responses.append({\"instruction\": instruction, \"positive\": positive, \"negative\": negative})\n",
    "        except:\n",
    "            print(\"Generation String Not Found! ID:\", idx)\n",
    "            # print(negative)\n",
    "    return all_responses\n",
    "\n",
    "def create_judge_response_prompts(instructions, positives, negatives, judging_template):\n",
    "    judge_prompts = []\n",
    "    labels = []\n",
    "    for instruction, positive, negative in zip(instructions, positives, negatives):\n",
    "        rand_num = np.random.rand()\n",
    "        if(rand_num < 0.5):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                    generation=positive, \n",
    "                                                                    generation2=negative)},\n",
    "            ]\n",
    "            judge_prompts.append(messages)\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": judging_template.format(input=instruction, \n",
    "                                                                    generation=negative, \n",
    "                                                                    generation2=positive)},\n",
    "            ]\n",
    "            judge_prompts.append(messages)\n",
    "            labels.append(0)\n",
    "    return judge_prompts, labels\n",
    "\n",
    "def load_textfile_as_string(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful AI assistant.<|end|>\n",
      "<|user|>\n",
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Begin your evaluation by first verifying whether each response contains any obvious or subtle errors. Then propose an appropriate evaluaiton rubric, e.g. 1-5 criteria that are important for evaluating responses to this specific user question. Continue your evaluation by checking each response carefully along those criteria. Based on the analysis in previous steps, choose which response is better overall. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your evaluation, output your final verdict by strictly following this format: \\\"[[A]]\\\" if assistant A is better, \\\"[[B]]\\\" if assistant B is better. \n",
      "\n",
      "[User Question]\n",
      "fast reading method when first few letters of words are bold\n",
      "\n",
      "[The Start of Assistant A's Answer]\n",
      "You're referring to a technique called \"Rapid Serial Visual Presentation\" (RSVP) with bolded first few letters! This method can indeed help you read faster and more efficiently. Here's how it works:\n",
      "\n",
      "1. Identify the first few letters of each word that are bolded. This can be 2-3 letters, depending on your preference.\n",
      "2. Focus on the bolded letters to quickly identify the word. This helps your brain to recognize the word more quickly and accurately.\n",
      "3. Move your eyes to the next word, repeating the process of identifying the bolded letters and recognizing the word.\n",
      "4. Continue this process at a rapid pace, moving your eyes from one word to the next.\n",
      "\n",
      "This technique can help you read faster in several ways:\n",
      "\n",
      "* Reduced eye movement: By focusing on the bolded letters, you reduce the amount of eye movement needed to read each word, allowing you to move more quickly through the text.\n",
      "* Increased recognition: The bolded letters help your brain to recognize words more quickly, as your brain is able to latch onto the familiar pattern of letters.\n",
      "* Improved processing: The bolded letters can also help your brain to process the words more efficiently, as the bolded letters provide a clear cue for your brain to recognize the word.\n",
      "\n",
      "To get the most out of this technique, try the following:\n",
      "\n",
      "* Start with a moderate pace and gradually increase your speed as you become more comfortable with the technique.\n",
      "* Practice with different texts and materials to get a feel for how the technique works with different types of writing.\n",
      "* Experiment with different bolding patterns, such as bolding the first letter of each word or the first letter of every other word, to find what works best for you.\n",
      "\n",
      "Remember, the key to success with this technique is to practice regularly and develop a consistent reading rhythm. With time and practice, you can improve your reading speed and comprehension using this method!\n",
      "[The End of Assistant A's Answer]\n",
      "\n",
      "[The Start of Assistant B's Answer]\n",
      "You're interested in learning a fast reading method that utilizes visual cues to improve recognition and processing! I'd be happy to help you with that.\n",
      "\n",
      "The method I'd like to introduce you to is called the \"Meta-Guiding\" technique. It's a visual cue-based method that can help you read faster and more efficiently.\n",
      "\n",
      "**What is Meta-Guiding?**\n",
      "\n",
      "Meta-Guiding is a technique that uses visual cues to guide your eyes as you read. The goal is to train your eyes to move more efficiently across the page, recognizing words and phrases more quickly, and processing the information more effectively.\n",
      "\n",
      "**How to Implement Meta-Guiding:**\n",
      "\n",
      "1. **Start by choosing a text**: Select a passage that you want to read quickly and efficiently. It can be a short article, a chapter, or even a book.\n",
      "2. **Identify the visual cues**: As you read, pay attention to the visual cues that catch your attention. These can be:\n",
      "\t* Headings and subheadings\n",
      "\t* Bold or italicized text\n",
      "\t* Images or graphics\n",
      "\t* Quotes or dialogue\n",
      "\t* Numbered or bulleted lists\n",
      "3. **Use these cues as guides**: As you read, use these visual cues to guide your eyes across the page. For example, if you see a heading, use it as a reference point to scan the surrounding text.\n",
      "4. **Practice active reading**: As you read, ask yourself questions like:\n",
      "\t* What is the main idea of this section?\n",
      "\t* What is the author trying to say?\n",
      "\t* What are the key points being made?\n",
      "5. **Use your peripheral vision**: Instead of focusing on individual words, use your peripheral vision to take in larger chunks of text. This will help you recognize patterns and relationships between words.\n",
      "6. **Practice, practice, practice!**: The more you practice using visual cues to guide your reading, the more efficient and effective you'll become.\n",
      "\n",
      "**Benefits of Meta-Guiding:**\n",
      "\n",
      "1. **Faster reading speeds**: By using visual cues, you can train your eyes to move more quickly across the page, allowing you to read faster.\n",
      "2. **Improved comprehension**: By focusing on the main ideas and key points, you'll be able to comprehend the text more effectively.\n",
      "3. **Reduced eye fatigue**: By using your peripheral vision and focusing on visual cues, you'll reduce eye fatigue and strain.\n",
      "4. **Increased retention**: By actively engaging with the text and using visual cues, you'll be able to retain more information.\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "1. **Start with short texts**: Begin with shorter texts and gradually move to longer ones as you become more comfortable with the technique.\n",
      "2. **Experiment with different types of text**: Try using Meta-Guiding with different types of text, such as articles, blog posts, or even social media updates.\n",
      "3. **Combine with other reading techniques**: You can combine Meta-Guiding with other reading techniques, such as skimming or scanning, to further improve your reading efficiency.\n",
      "\n",
      "I hope this helps you get started with the Meta-Guiding technique! Remember to practice regularly and have fun exploring the benefits of this visual cue-based method.\n",
      "[The End of Assistant B's Answer]<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Begin your evaluation by first verifying whether each response contains any obvious or subtle errors. Then propose an appropriate evaluaiton rubric, e.g. 1-5 criteria that are important for evaluating responses to this specific user question. Continue your evaluation by checking each response carefully along those criteria. Based on the analysis in previous steps, choose which response is better overall. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your evaluation, output your final verdict by strictly following this format: \\\\\\\"[[A]]\\\\\\\" if assistant A is better, \\\\\\\"[[B]]\\\\\\\" if assistant B is better. \\n\\n[User Question]\\nfast reading method when first few letters of words are bold\\n\\n[The Start of Assistant A's Answer]\\nYou're referring to a technique called \\\"Rapid Serial Visual Presentation\\\" (RSVP) with bolded first few letters! This method can indeed help you read faster and more efficiently. Here's how it works:\\n\\n1. Identify the first few letters of each word that are bolded. This can be 2-3 letters, depending on your preference.\\n2. Focus on the bolded letters to quickly identify the word. This helps your brain to recognize the word more quickly and accurately.\\n3. Move your eyes to the next word, repeating the process of identifying the bolded letters and recognizing the word.\\n4. Continue this process at a rapid pace, moving your eyes from one word to the next.\\n\\nThis technique can help you read faster in several ways:\\n\\n* Reduced eye movement: By focusing on the bolded letters, you reduce the amount of eye movement needed to read each word, allowing you to move more quickly through the text.\\n* Increased recognition: The bolded letters help your brain to recognize words more quickly, as your brain is able to latch onto the familiar pattern of letters.\\n* Improved processing: The bolded letters can also help your brain to process the words more efficiently, as the bolded letters provide a clear cue for your brain to recognize the word.\\n\\nTo get the most out of this technique, try the following:\\n\\n* Start with a moderate pace and gradually increase your speed as you become more comfortable with the technique.\\n* Practice with different texts and materials to get a feel for how the technique works with different types of writing.\\n* Experiment with different bolding patterns, such as bolding the first letter of each word or the first letter of every other word, to find what works best for you.\\n\\nRemember, the key to success with this technique is to practice regularly and develop a consistent reading rhythm. With time and practice, you can improve your reading speed and comprehension using this method!\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\nYou're interested in learning a fast reading method that utilizes visual cues to improve recognition and processing! I'd be happy to help you with that.\\n\\nThe method I'd like to introduce you to is called the \\\"Meta-Guiding\\\" technique. It's a visual cue-based method that can help you read faster and more efficiently.\\n\\n**What is Meta-Guiding?**\\n\\nMeta-Guiding is a technique that uses visual cues to guide your eyes as you read. The goal is to train your eyes to move more efficiently across the page, recognizing words and phrases more quickly, and processing the information more effectively.\\n\\n**How to Implement Meta-Guiding:**\\n\\n1. **Start by choosing a text**: Select a passage that you want to read quickly and efficiently. It can be a short article, a chapter, or even a book.\\n2. **Identify the visual cues**: As you read, pay attention to the visual cues that catch your attention. These can be:\\n\\t* Headings and subheadings\\n\\t* Bold or italicized text\\n\\t* Images or graphics\\n\\t* Quotes or dialogue\\n\\t* Numbered or bulleted lists\\n3. **Use these cues as guides**: As you read, use these visual cues to guide your eyes across the page. For example, if you see a heading, use it as a reference point to scan the surrounding text.\\n4. **Practice active reading**: As you read, ask yourself questions like:\\n\\t* What is the main idea of this section?\\n\\t* What is the author trying to say?\\n\\t* What are the key points being made?\\n5. **Use your peripheral vision**: Instead of focusing on individual words, use your peripheral vision to take in larger chunks of text. This will help you recognize patterns and relationships between words.\\n6. **Practice, practice, practice!**: The more you practice using visual cues to guide your reading, the more efficient and effective you'll become.\\n\\n**Benefits of Meta-Guiding:**\\n\\n1. **Faster reading speeds**: By using visual cues, you can train your eyes to move more quickly across the page, allowing you to read faster.\\n2. **Improved comprehension**: By focusing on the main ideas and key points, you'll be able to comprehend the text more effectively.\\n3. **Reduced eye fatigue**: By using your peripheral vision and focusing on visual cues, you'll reduce eye fatigue and strain.\\n4. **Increased retention**: By actively engaging with the text and using visual cues, you'll be able to retain more information.\\n\\n**Tips and Variations:**\\n\\n1. **Start with short texts**: Begin with shorter texts and gradually move to longer ones as you become more comfortable with the technique.\\n2. **Experiment with different types of text**: Try using Meta-Guiding with different types of text, such as articles, blog posts, or even social media updates.\\n3. **Combine with other reading techniques**: You can combine Meta-Guiding with other reading techniques, such as skimming or scanning, to further improve your reading efficiency.\\n\\nI hope this helps you get started with the Meta-Guiding technique! Remember to practice regularly and have fun exploring the benefits of this visual cue-based method.\\n[The End of Assistant B's Answer]\"\n",
    "            }\n",
    "        ]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt_token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(prompt_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-30 18:33:03 config.py:383] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-30 18:33:03 llm_engine.py:232] Initializing an LLM engine (v0.6.1) with config: model='microsoft/Phi-3.5-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/Phi-3.5-mini-instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "INFO 12-30 18:33:04 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 12-30 18:33:04 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pradyumnachari/anaconda3/envs/llm/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/pradyumnachari/anaconda3/envs/llm/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-30 18:33:05 model_runner.py:997] Starting to load model microsoft/Phi-3.5-mini-instruct...\n",
      "INFO 12-30 18:33:06 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 12-30 18:33:06 selector.py:116] Using XFormers backend.\n",
      "INFO 12-30 18:33:06 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c60a37e061456281dff9385d4ecbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-30 18:33:08 model_runner.py:1008] Loading model weights took 7.1659 GB\n",
      "INFO 12-30 18:33:09 gpu_executor.py:122] # GPU blocks: 2325, # CPU blocks: 682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=2000)\n",
    "llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, \n",
    "          trust_remote_code=True, enforce_eager=True, gpu_memory_utilization=0.90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me a brief history of the Roman Empire.\"\n",
    "question = \"What are the benefits of stretching?\"\n",
    "question = \"Write me a python function that outputs the number of parameters in a pytorch model.\"\n",
    "o_question = \"A train leaves Station A at 8:00 AM, traveling at 60 km/h. Another train departs Station B, 120 km away from Station A, at 9:00 AM, traveling toward Station A at 80 km/h. At what time do the two trains meet?\"\n",
    "# o_question = \"\"\"The island of Solara is home to a unique species of bird called the Solara Finch. These finches primarily feed on a specific type of berry that grows on tall bushes. During the dry season, the berry supply diminishes, forcing the finches to travel farther to find food. Researchers have observed that during the dry season, the finches’ wing muscles become stronger, allowing them to fly greater distances. This adaptation enables them to survive harsh conditions. However, researchers are concerned that prolonged dry seasons due to climate change may outpace the finches' ability to adapt.\n",
    "\n",
    "# Question: Based on the passage, which of the following is the most likely outcome if the dry season becomes significantly longer over time?\n",
    "\n",
    "# A) The Solara Finch population will increase due to improved flight capabilities.\n",
    "# B) The Solara Finch population may decline because they might not adapt quickly enough.\n",
    "# C) The finches will switch to a diet of insects to compensate for the loss of berries.\n",
    "# D) The finches will migrate permanently to areas where berries are abundant year-round.\"\"\"\n",
    "# o_question = \"\"\"In recent years, the rapid advancement of artificial intelligence (AI) has sparked debates about its ethical implications and societal impact. Proponents argue that AI can solve complex problems, from predicting climate patterns to diagnosing diseases, potentially improving countless lives. Critics, however, warn of potential job displacement, biased algorithms, and privacy concerns. Some researchers advocate for stronger regulatory frameworks to ensure AI development aligns with human values, while others fear that excessive regulation may stifle innovation and delay technological progress.\n",
    "\n",
    "# Question:\n",
    "# Discuss the potential trade-offs involved in implementing stricter regulations on artificial intelligence development, considering both ethical and societal perspectives.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.56s/it, est. speed input: 14.93 toks/s, output: 53.42 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let's break down this problem step-by-step:\n",
      "\n",
      "1. In one hour (from 8:00 AM to 9:00 AM), the first train travels 60 km.\n",
      "2. The distance between the two trains after the first train has traveled for one hour is now 120 km - 60 km = 60 km.\n",
      "3. Now, both trains are moving towards each other. Their combined speed is 60 km/h + 80 km/h = 140 km/h.\n",
      "4. To cover the remaining 60 km between them, they need to travel at a combined speed of 140 km/h.\n",
      "5. To find out how long it takes them to cover the remaining distance, we can use the formula: time = distance / speed.\n",
      "6. In this case, time = 60 km / 140 km/h = 0.4286 hours (approximately).\n",
      "7. Since the second train left at 9:00 AM, they will meet around 9:00 AM + 0.4286 hours = 9:26 AM.\n",
      "\n",
      "So, the two trains meet at approximately 9:26 AM.\n",
      "\n",
      "Final answer: The two trains meet at around 9:26 AM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated_responses_pos_prompts = [[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": o_question\n",
    "            }\n",
    "        ]]\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "generated_responses_pos = [output.outputs[0].text for output in outputs]\n",
    "print(generated_responses_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.16s/it, est. speed input: 45.46 toks/s, output: 50.67 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The response provided by the AI assistant contains a calculation error in determining the meeting time of the two trains. Let's correct this and evaluate the response based on the following criteria:\n",
      "\n",
      "1. Accuracy of calculations and logic applied\n",
      "2. Clarity and coherence in the explanation\n",
      "3. Relevance and precision of the information provided\n",
      "4. Correctness of rounding (if applicable)\n",
      "\n",
      "Revised Calculation:\n",
      "\n",
      "1. From 8:00 AM to 9:00 AM, the first train covers 60 km. The remaining distance between the two trains is now 120 km - 60 km = 60 km.\n",
      "2. The relative speed at which the two trains are approaching each other is 60 km/h + 80 km/h = 140 km/h.\n",
      "3. To find out how long it takes for the two trains to cover the remaining 60 km at a combined speed of 140 km/h, we use the formula: Time = Distance / Speed.\n",
      "4. Time = 60 km / 140 km/h ≈ 0.43 hours (approximately 26 minutes).\n",
      "5. Since the second train started at 9:00 AM, adding approximately 26 minutes to this time gives us 9:26 AM.\n",
      "6. During this time, the first train has also been moving at 60 km/h for 26 minutes, covering a distance of 60 km/h * (26/60) hours = 26 km.\n",
      "7. Now, the distance between the two trains is reduced to 60 km - 26 km = 34 km.\n",
      "8. Continuing to move at a combined speed of 140 km/h, it will take the trains an additional (34 km) / (140 km/h) ≈ 0.24 hours (approximately 14 minutes) to cover this remaining distance.\n",
      "9. Adding these 14 minutes to the previous time of 9:26 AM gives us a final meeting time of approximately 9:30 AM.\n",
      "\n",
      "Corrected Meeting Time: The two trains will meet around 9:30 AM.\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "1. Accuracy of calculations and logic: The initial error was in rounding the meeting time to 10:00 AM. The corrected calculations accurately determine that the two trains will meet around 9:30 AM.\n",
      "2. Clarity and coherence: The explanation provided is clear and logical, with each step being explained thoroughly.\n",
      "3. Relevance and precision of the information: The response is relevant to the question and provides precise information regarding the meeting time.\n",
      "4. Correctness of rounding: The final rounded meeting time of 10:00 AM should have been adjusted to the more precise time of 9:30 AM based on the calculations.\n",
      "\n",
      "Final Evaluation: With the corrected calculations, the response is accurate, clear, coherent, and relevant. The rounding issue was corrected, making the response suitable for evaluating the user question. The initial rating would have been incorrect due to the rounding error, but with the adjustments made, the response now aligns well with the evaluation criteria and can be rated highly, perhaps a 4.5 out of 5, considering that rounding to a whole hour was not necessary and the answer should be precise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Judgement\n",
    "judgement_prompt = load_textfile_as_string(\"../prompts/judgement.prompt\")\n",
    "formatted_prompt = judgement_prompt.format(question=o_question, response=generated_responses_pos[0])\n",
    "\n",
    "generated_responses_pos_prompts = [[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_prompt\n",
    "            }\n",
    "        ]]\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "outs = [output.outputs[0].text for output in outputs]\n",
    "judgement = outs[0]\n",
    "print(judgement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.85s/it, est. speed input: 48.42 toks/s, output: 51.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The response provided contains a logical flow and attempts to address the problem with relevant calculations. However, there are mistakes in the calculation and final time estimation. Here's an evaluation of the response:\n",
      "\n",
      "1. Clarity: The explanation is generally clear, but there are some calculation errors.\n",
      "2. Conciseness: The response is concise but includes unnecessary approximations.\n",
      "3. Precision and Accuracy: The calculation lacks precision. The error occurs when the trains' combined distance traveled is incorrectly calculated.\n",
      "4. Relevance: The response stays relevant to the question throughout.\n",
      "5. Engagement: The response does not engage the reader with any interesting facts or context.\n",
      "6. Depth and Breadth: The response covers the basic steps but lacks depth in explanation and does not explore alternative methods or potential pitfalls.\n",
      "7. Logical Flow: The logical flow is disrupted by the calculation errors.\n",
      "8. Tone and Objectivity: The tone is informative, but the response lacks objectivity due to errors.\n",
      "9. Style: The style is straightforward and appropriate for a mathematical problem.\n",
      "10. Critical Thinking: The response lacks critical thinking due to the mistakes in calculations.\n",
      "11. Creativity: There is no creativity involved in the response.\n",
      "\n",
      "Revised response with correct calculations:\n",
      "\n",
      "The two trains will meet at approximately 9:43 AM.\n",
      "\n",
      "Here's the corrected reasoning:\n",
      "\n",
      "1. From 8:00 AM to 9:00 AM, the first train travels 60 km (at 60 km/h).\n",
      "2. At 9:00 AM, the distance between the two trains is 120 km - 60 km = 60 km.\n",
      "3. The relative speed at which the two trains are approaching each other is 60 km/h + 80 km/h = 140 km/h.\n",
      "4. To find out how long it takes for the two trains to cover the 60 km at a combined speed of 140 km/h, we use the formula: Time = Distance / Speed.\n",
      "5. Time = 60 km / 140 km/h ≈ 0.43 hours (approximately 26 minutes).\n",
      "6. Since the second train started at 9:00 AM, adding approximately 26 minutes gives us 9:26 AM.\n",
      "7. During this time, the first train continues to travel. In 26 minutes, the first train travels 60 km/h * (26/60) hours = 26 km.\n",
      "8. The remaining distance between the two trains is 60 km - 26 km = 34 km.\n",
      "9. To cover the 34 km at a combined speed of 140 km/h, it will take (34 km) / (140 km/h) ≈ 0.24 hours (approximately 14 minutes).\n",
      "10. Adding these 14 minutes to 9:26 AM gives us 9:30 AM.\n",
      "\n",
      "By carefully checking the calculations, we find that the two trains will meet at 9:30 AM, which is more accurate than the initial estimate of 10:00 AM. The revised response maintains the concise style while correcting the mistakes, resulting in a precise and accurate answer. No changes to the length were required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "suggestion_prompt = load_textfile_as_string(\"../prompts/suggest.prompt\")\n",
    "generated_responses_pos_prompts = [[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": suggestion_prompt.format(question=o_question, response=generated_responses_pos[0])\n",
    "            }\n",
    "        ]]\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "generated_responses = [output.outputs[0].text for output in outputs]\n",
    "print(generated_responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.05s/it, est. speed input: 3.75 toks/s, output: 52.50 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The implementation of stricter regulations on artificial intelligence (AI) development involves complex ethical and societal trade-offs, particularly concerning job displacement and economic impact, as well as the potential effects on innovation and global competitiveness in the tech industry. Here are some key considerations:\n",
      "\n",
      "### Ethical and Societal Trade-offs\n",
      "\n",
      "**1. Job Displacement:**\n",
      "- **Pros:**\n",
      "    - Strict regulations may lead to the development of more ethically aligned AI systems that prioritize human-centric values and thus, potentially create jobs in ethically compliant sectors.\n",
      "    - Regulations might encourage a shift towards higher-skilled positions as routine tasks are automated.\n",
      "- **Cons:**\n",
      "    - AI can perform a wide range of tasks with efficiency and precision, leading to significant job displacement in various industries.\n",
      "    - The immediate loss of jobs may exacerbate economic inequality and cause hardship for individuals and communities heavily reliant on routine, low-skilled positions.\n",
      "\n",
      "**2. Economic Impact:**\n",
      "- **Pros:**\n",
      "    - Regulations can lead to a more stable and trustworthy AI market, thereby attracting investment in secure and reliable AI applications.\n",
      "    - In the long term, such regulations could prevent costly societal issues, like systemic bias, privacy violations, and security breaches, potentially saving on costs associated with these problems.\n",
      "- **Cons:**\n",
      "    - Overregulation may stifle AI innovation, leading to reduced economic growth, lower productivity, and potentially higher costs for consumers.\n",
      "    - Businesses may move operations to countries with more lenient regulations, affecting local economies and employment rates.\n",
      "\n",
      "### Effects on Innovation and Global Competitiveness\n",
      "\n",
      "**1. Innovation:**\n",
      "- **Positive Impact:**\n",
      "    - By setting clear ethical standards, regulations can spur innovation by pushing companies to find new ways to comply, potentially leading to breakthroughs in responsible AI technologies.\n",
      "    - They could also prevent the development and deployment of harmful AI systems, which might otherwise have prompted innovation driven by evasion.\n",
      "- **Negative Impact:**\n",
      "    - Overly stringent regulations might deter researchers and developers from exploring new applications of AI due to fear of legal repercussions or excessive bureaucracy.\n",
      "    - A slowdown in innovation can lead to a technological gap with countries that are less regulated, causing a potential loss of influence in setting global AI standards.\n",
      "\n",
      "**2. Global Competitiveness:**\n",
      "- **Positive Impact:**\n",
      "    - A robust regulatory framework can foster trust in the domestic market, attracting international companies and talent, thereby enhancing competitiveness.\n",
      "    - Countries that are viewed as leaders in ethical AI development may set global norms and standards, gaining a strategic advantage.\n",
      "- **Negative Impact:**\n",
      "    - Rigid regulations could make a country's tech industry less agile and less competitive globally, as international firms might prefer more permissive environments.\n",
      "    - A lag in AI advancements due to regulatory hurdles may result in missed opportunities for economic growth and development, leading to a downward spiral in global rankings.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Balancing the ethical and societal considerations with the need to foster innovation and maintain global competitiveness is a delicate task. A collaborative approach involving stakeholders from various sectors (government, academia, industry, and civil society) is critical to developing regulations that are both ethically sound and conducive to innovation. Policies should be designed to protect workers, support the transition to a more advanced and skilled workforce, and maintain the economic advantages of the tech industry while ensuring that AI developments are aligned with societal values and human well-being.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me a detailed analysis of the political structure and key events during the Roman Republic (509-27 BCE).\"\n",
    "question = \"A train leaves Station A at 8:00 AM, traveling at 60 km/h. Another train departs Station B, 120 km away from Station A, at 9:00 AM, traveling towards Station A at 80 km/h. At what time will the two trains meet, rounded to the nearest minute?\"\n",
    "question = \"Discuss the ethical and societal trade-offs specifically related to job displacement and economic impact when implementing stricter regulations on artificial intelligence development. How might these regulations affect innovation and global competitiveness in the tech industry?\"\n",
    "generated_responses_pos_prompts = [[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]]\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "generated_responses_pos = [output.outputs[0].text for output in outputs]\n",
    "print(generated_responses_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.30s/it, est. speed input: 94.09 toks/s, output: 50.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The provided response effectively addresses the question by outlining several key trade-offs involved in implementing stricter regulations on artificial intelligence (AI) development. The response is structured and touches upon the ethical and societal perspectives requested. Here is an evaluation using a proposed rubric:\n",
      "\n",
      "**Rubric for Evaluation:**\n",
      "\n",
      "1. **Clarity of Expression:**\n",
      "   - The response is well-articulated and clear.\n",
      "   - Criteria: 5/5\n",
      "\n",
      "2. **Depth of Analysis:**\n",
      "   - The response explores various dimensions of the trade-offs involved, including job displacement, economic impact, innovation, and global competitiveness.\n",
      "   - Criteria: 5/5\n",
      "\n",
      "3. **Relevance to Question:**\n",
      "   - The response directly addresses the potential trade-offs in stricter AI regulations from ethical and societal perspectives.\n",
      "   - Criteria: 5/5\n",
      "\n",
      "4. **Consideration of Multiple Perspectives:**\n",
      "   - The response acknowledges both the advantages and disadvantages, offering a balanced view.\n",
      "   - Criteria: 5/5\n",
      "\n",
      "5. **Practical Suggestions for Balancing Trade-offs:**\n",
      "   - The response provides actionable suggestions for policymakers and industry leaders to balance the trade-offs, such as phased regulations and international dialogues.\n",
      "   - Criteria: 5/5\n",
      "\n",
      "6. **Use of Evidence and Examples:**\n",
      "   - While the response is comprehensive, it lacks specific examples or data to illustrate points, which could strengthen its argument.\n",
      "   - Criteria: 3/5\n",
      "\n",
      "7. **Conciseness and Precision:**\n",
      "   - The response is relatively concise and stays on topic throughout.\n",
      "   - Criteria: 4/5\n",
      "\n",
      "8. **Ethical Reasoning:**\n",
      "   - The response demonstrates ethical reasoning by considering the implications of AI on human values, job displacement, and economic models.\n",
      "   - Criteria: 5/5\n",
      "\n",
      "**Overall Evaluation:**\n",
      "\n",
      "The response to the question is of high quality. It effectively uses relevant criteria to discuss the trade-offs of implementing stricter AI regulations. The response is clear, balanced, and well-reasoned, offering practical insights into the complex interplay between ethical considerations and societal impacts. The only aspect that could be improved is the inclusion of specific examples or data to support the analysis, which would provide additional credibility to the points made.\n",
      "\n",
      "**Aggregate Score (Average of all criteria): 4.5/5**\n",
      "\n",
      "The evaluation concludes that the response is comprehensive, insightful, and well-structured, offering a nuanced understanding of the ethical and societal trade-offs associated with stricter AI regulations. With minor improvements in providing evidence and examples, this response could be even more persuasive and informative.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Judgement\n",
    "judgement_prompt = load_textfile_as_string(\"../prompts/judgement.prompt\")\n",
    "formatted_prompt = judgement_prompt.format(question=o_question, response=generated_responses_pos[0])\n",
    "\n",
    "generated_responses_pos_prompts = [[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": formatted_prompt\n",
    "            }\n",
    "        ]]\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "outs = [output.outputs[0].text for output in outputs]\n",
    "judgement = outs[0]\n",
    "print(judgement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.77s/it, est. speed input: 324.45 toks/s, output: 49.65 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The response is well-structured and directly addresses the potential trade-offs involved in implementing stricter regulations on AI from both ethical and societal perspectives. The response clearly outlines the pros and cons in each area, providing a comprehensive overview of the complexities at play. To ensure that the response aligns precisely with the user question, the structure and scope are well-maintained, with a balanced view that considers both the benefits and drawbacks of regulation.\n",
      "\n",
      "No change to the user question is necessary as it effectively prompts a discussion of the trade-offs associated with AI regulation, which the response thoroughly explores. The question is clear and well-suited to elicit the provided detailed analysis of ethical and societal considerations. The response fully satisfies the requirements of the user's query without needing further modification.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generated_responses_pos_prompts = [[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful AI assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": suggestion_prompt.format(question=o_question, response=generated_responses_pos[0])\n",
    "            }\n",
    "        ]]\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "generated_responses = [output.outputs[0].text for output in outputs]\n",
    "print(generated_responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "#dataset is a subset of WildChat\n",
    "dataset = load_dataset(\"facebook/Self-taught-evaluator-DPO-data\")\n",
    "WildChat = load_dataset(\"allenai/WildChat-1M\")\n",
    "\n",
    "# get hashes\n",
    "#load the hash_id2content dictionary\n",
    "# with open(\"hash_id2content.pkl\", \"rb\") as f:\n",
    "#   hash_id2content = pickle.load(f)\n",
    "hash_id2content = dict()\n",
    "for ex in tqdm(WildChat[\"train\"], total=len(WildChat[\"train\"])):\n",
    "  turn = ex[\"turn\"]\n",
    "  hash_id2content[ex[\"conversation_hash\"]] = ex[\"conversation\"][2 * (turn - 1)][\"content\"]\n",
    "\n",
    "\n",
    "print(\"Starting 2.\")\n",
    "train_data = []\n",
    "for ex in tqdm(dataset[\"train\"], total=len(dataset[\"train\"])):\n",
    "  if ex[\"instruction\"] not in hash_id2content:\n",
    "    continue\n",
    "  else:\n",
    "    ex[\"src\"] = ex[\"src\"].replace(ex[\"instruction\"], hash_id2content[ex[\"instruction\"]])\n",
    "    train_data.append(ex)\n",
    "\n",
    "print(\"Starting 3.\")\n",
    "# Extract Instructions\n",
    "skip = 6 # I found that the dataset instructions has the same instruction repeated 6 times? Not sure why. \n",
    "all_instructions = []\n",
    "num_responses = len(dataset['train'])//skip\n",
    "for i in tqdm(range(0, num_responses*skip, skip), total=num_responses):\n",
    "    try:\n",
    "        instruction_example = hash_id2content[dataset['train'][i]['instruction']]\n",
    "        all_instructions.append(instruction_example)\n",
    "    except:\n",
    "        continue\n",
    "print(all_instructions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_response_prompt_template = load_textfile_as_string('./prompts/worse_response_v2.prompt')\n",
    "print(worse_response_prompt_template)\n",
    "judging_prompt_template = load_textfile_as_string('./prompts/eval_plan.prompt')\n",
    "print(judging_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses_pos_prompts = get_response_prompts(all_instructions)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_pos_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_responses_pos = [output.outputs[0].text for output in outputs]\n",
    "print(generated_responses_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions_pos_neg = get_worse_response_question_prompts(all_instructions, generated_responses_pos, worse_response_prompt_template)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in all_instructions_pos_neg]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "all_instructions_neg = [output.outputs[0].text for output in outputs]\n",
    "print(all_instructions_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_all_instructions, f_generated_responses_pos, f_all_instructions_neg = extract_negative_questions(all_instructions, generated_responses_pos, all_instructions_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_instructions), len(f_all_instructions), len(f_generated_responses_pos), len(f_all_instructions_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_to_judge = extract_negative_responses(all_instructions, generated_responses_pos, generated_responses_neg)\n",
    "# print(items_to_judge[0])\n",
    "# print(len(items_to_judge))\n",
    "\n",
    "generated_responses_neg_prompts = get_response_prompts(f_all_instructions_neg)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in generated_responses_neg_prompts]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "f_generated_responses_neg = [output.outputs[0].text for output in outputs]\n",
    "print(f_generated_responses_neg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_judgements, labels = create_judge_response_prompts(f_all_instructions, f_generated_responses_pos, f_generated_responses_neg, judging_prompt_template)\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True) for messages in all_judgements]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_judgements = [output.outputs[0].text for output in outputs]\n",
    "print(generated_judgements[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(f_all_instructions[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_generated_responses_pos[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_all_instructions_neg[idx])\n",
    "print(\"_____\"*10)\n",
    "print(f_generated_responses_neg[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = 0\n",
    "A_correct = []\n",
    "B_correct = []\n",
    "count_correct_format_incorrect_answer = 0\n",
    "for judge_prompt, judgement, label in zip(all_judgements, generated_judgements, labels):\n",
    "    if((\"[[A]]\" in judgement and label == 1) or (\"[[B]]\" in judgement and label == 0)):\n",
    "        count_correct += 1\n",
    "        if(\"[[A]]\" in judgement):\n",
    "            A_correct.append([{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "                                {\"role\": \"user\", \"content\": judge_prompt[1][\"content\"]},\n",
    "                                {\"role\": \"assistant\", \"content\": judgement}])\n",
    "        else:\n",
    "            B_correct.append([{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "                                {\"role\": \"user\", \"content\": judge_prompt[1][\"content\"]},\n",
    "                                {\"role\": \"assistant\", \"content\": judgement}])\n",
    "    else:\n",
    "        # print(judgement)\n",
    "        # print(\"*************\")\n",
    "        if(\"[[A]]\" in judgement or \"[[B]]\" in judgement):\n",
    "            count_correct_format_incorrect_answer += 1\n",
    "print(\"Correct\", count_correct, \"/\", len(all_judgements))\n",
    "print(\"A correct: \", len(A_correct))\n",
    "print(\"B correct: \", len(B_correct))\n",
    "print(\"Formatted Incorrect: \", count_correct_format_incorrect_answer, \"/\", len(all_judgements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "import random\n",
    "if(len(A_correct) > len(B_correct)):\n",
    "    A_correct_sampled = random.sample(A_correct, len(B_correct))\n",
    "    B_correct_sampled = B_correct\n",
    "else:\n",
    "    B_correct_sampled = random.sample(B_correct, len(A_correct))\n",
    "    A_correct_sampled = A_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(A_correct_sampled), len(B_correct_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate token statistics for the generated responses\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_token_statistics(messages):\n",
    "    token_counts = []\n",
    "    for message in messages:\n",
    "        tokens = tokenizer.apply_chat_template(message, add_generation_prompt=False, tokenize=True)\n",
    "        token_counts.append(len(tokens))\n",
    "    print(\"Mean Tokens: \", np.mean(token_counts))\n",
    "    print(\"Median Tokens: \", np.median(token_counts))\n",
    "    print(\"Max Tokens: \", np.max(token_counts))\n",
    "    print(\"Min Tokens: \", np.min(token_counts))\n",
    "    print(\"STD Tokens: \", np.std(token_counts))\n",
    "\n",
    "calculate_token_statistics(A_correct_sampled+ B_correct_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "import json\n",
    "\n",
    "all_sft_samples = A_correct_sampled + B_correct_sampled\n",
    "json_array = []\n",
    "for i, judgement in enumerate(all_sft_samples):\n",
    "    json_array.append({\"messages\": judgement})\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = 'all_judgements_llama.json'\n",
    "\n",
    "# Write the list of dictionaries to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_array, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
